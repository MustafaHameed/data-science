Week 1
-----------------------------

PMF - Probability mass function
Example:
    P(x) = p^x * (1 - p)^(1 - x) (x = 0, 1)
    P(0) = p
    P(1) = 1 - p

PDF - Probability density function
    - Associated with continuous random variable
Example:
    f(x) = 2x for 0<x<1, 0 elsewhere

CDF and survival function:
    - Cumulative distribution function: F(x) = P(X <= x)
        F(0,75) = pbeta(0.75, 2, 1)
    - Survival function: S(x) = P(X > x) = 1 - F(x)

Quantile:
    The a-th quantile of a distribution function F is the point xa so that
    F(xa) = a
    - Percentile is a quantile with a expressed as a percent
    - Median is the 50th percentile

Conditional probability: P(A|B) = P(A&B)/P(B) probability of A, given B

Bayes' Rule:
P(B|A) = (P(A|B)P(B))/(P(A|B)P(B) + P(A|Bˆc)P(Bˆc))
- need Probability of B marginalize over A
- P(A|Bˆc) = 1 - P(Aˆc|Bˆc)

likelihood ratios:
P(D|+) / P(Dˆc|+) = (P(+|D) / P(+|Dˆc)) * ((P(D) / P(Dˆc)))
post-test odds of D = DLR+ * pre-test odds of D

IID random variables: independent and identically distributed

week 2
-----------------------------
Variance:
---------
Var(X) = E[(X-µ)ˆ2] = E[Xˆ2] - E[x]ˆ2

The sample variance
-------------------
S^2 = (∑{i=1} (X_i - \bar X)^2)/(n-1)
(almost, but not quite, the average squared deviation from the sample mean)
    - It is also a random variable
    - Its expected value is the population variance
    - Its distribution gets more concentrated around the population variance with more data
    - Its square root is the sample standard deviation

- The sample variance, S^2, estimates the population variance, sigma^2
- The distribution of the sample variance is centered around sigma^2
- The variance of the sample mean is sigma^2 / n
    - Its logical estimate is s^2 / n
    - The logical estimate of the standard error is S / sqrt{n}
- S, the standard deviation, talks about how variable the population is
- S/sqrt{n}, the standard error, talks about how variable averages of random
samples of size n from the population are

Simulation example:
-------------------
Standard normals have variance 1; means of $n$ standard normals have standard
deviation 1/sqrt{n}
    nosim <- 1000
    n <- 10
    sd(apply(matrix(rnorm(nosim * n), nosim), 1, mean))
    ## [1] 0.3156
    1 / sqrt(n)
    ## [1] 0.3162

Standard uniforms have variance $1/12$; means of random samples of $n$ uniforms
have sd 1/sqrt{12 * n}
    nosim <- 1000
    n <- 10
    sd(apply(matrix(runif(nosim * n), nosim), 1, mean))
    ## [1] 0.09017
    1 / sqrt(12 * n)
    ## [1] 0.09129

Poisson(4) have variance $4$; means of random samples of $n$ Poisson(4) have sd
2/sqrt{n}
    nosim <- 1000
    n <- 10
    sd(apply(matrix(rpois(nosim * n, 4), nosim), 1, mean))
    ## [1] 0.6219
    2 / sqrt(n)
    ## [1] 0.6325

Fair coin flips have variance $0.25$; means of random samples of $n$ coin flips
have sd 1 / (2 sqrt{n})
    nosim <- 1000
    n <- 10
    sd(apply(matrix(sample(0 : 1, nosim * n, replace = TRUE),
                    nosim), 1, mean))
    ## [1] 0.1587
    1 / (2 * sqrt(n))
    ## [1] 0.1581

The Bernoulli distribution:
---------------------------
- take (only) the values 1 and 0 with probabilities of p and 1-p respectively
- The PMF for a Bernoulli random variable X is P(X = x) = p^x (1 - p)^{1 - x}
- The mean of a Bernoulli random variable is p and the variance is p(1 - p)
- If we let X be a Bernoulli random variable, it is typical to call X=1 as a
"success" and X=0 as a "failure"

The Binomial distribution:
--------------------------
- The binomial random variables are obtained as the sum of iid Bernoulli trials
- Let X_1,...,X_n be iid Bernoulli(p); then X = ∑{i=1~n}X_i is a binomial random variable
- The binomial mass function: P(X=x) = pˆx * (1-p)ˆ(n-x)

Example:
Suppose a friend has 8 children, 7 of which are girls and none are twins, if each
gender has an independent 50% probability for each birth, what's the probability
of getting 7 or more girls out of 8 births?
    choose(8, 7) * 0.5^8 + choose(8, 8) * 0.5^8
    ## [1] 0.03516
    pbinom(6, size = 8, prob = 0.5, lower.tail = FALSE)
    ## [1] 0.03516

The normal distribution:
------------------------
Facts about the normal density:
    - If X \sim \mbox{N}(\mu,\sigma^2) then Z = \frac{X -\mu}{\sigma} \sim N(0, 1)
    - If Z is standard normal X = \mu + \sigma Z \sim \mbox{N}(\mu, \sigma^2)
    - Approximately 68%, 95% and 99% of the normal density lies within 1, 2 and
    3 standard deviations from the mean, respectively
    - -1.28, $-1.645, -1.96 and -2.33 are the 10th, 5th, 2.5th and 1st percentiles
    of the standard normal distribution respectively
    - By symmetry, 1.28, 1.645, 1.96 and 2.33 are the 90th}, 95th, 97.5th and 99th
    percentiles of the standard normal distribution respectively
    - In general mu + sigma z_0 where z_0 is the appropriate standard normal quantile

Example:
Assume that the number of daily ad clicks for a company is (approximately) normally
distributed with a mean of 1020 and a standard deviation of 50. What's the probability
of getting more than 1,160 clicks in a day?
    pnorm(1160, mean = 1020, sd = 50, lower.tail = FALSE)
    ## [1] 0.002555
    pnorm(2.8, lower.tail = FALSE)
    ## [1] 0.002555

The Poisson distribution:
--------------------------
- Used to model counts
- The Poisson mass function is P(X = x; lambda) = (lambda^x * e^(-lambda))/x! for x=0,1,...
- The mean of this distribution is lambda
- The variance of this distribution is lambda
- Notice that x ranges from 0 to infty

Some uses for the Poisson distribution:
    - Modeling count data
    - Modeling event-time or survival data
    - Modeling contingency tables
    - Approximating binomials when n is large and p is small

Rates and Poisson random variables:
    - Poisson random variables are used to model rates
    - X \sim Poisson(lambda t) where
        - lambda = E[X / t] is the expected count per unit of time
        - t is the total monitoring time
Example:
    The number of people that show up at a bus stop is Poisson with a mean of $2.5$
    per hour.
    If watching the bus stop for 4 hours, what is the probability that $3$ or fewer
    people show up for the whole time?
        ppois(3, lambda = 2.5 * 4)
        ## [1] 0.01034

Example, Poisson approximation to the binomial/;
    We flip a coin with success probablity 0.01 five hundred times.
    What's the probability of 2 or fewer successes?
        pbinom(2, size = 500, prob = 0.01)
        ## [1] 0.1234
        ppois(2, lambda = 500 * 0.01)
        ## [1] 0.1247

Asymptotics and Law of Large Numbers:
------------------------------------
Law of large numbers in action:
    n <- 10000
    means <- cumsum(rnorm(n))/(1:n)
    library(ggplot2)
    g <- ggplot(data.frame(x = 1:n, y = means), aes(x = x, y = y))
    g <- g + geom_hline(yintercept = 0) + geom_line(size = 2)
    g <- g + labs(x = "Number of obs", y = "Cumulative mean")
    g
